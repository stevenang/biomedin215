{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3 Cohort Building üë•\n",
    "\n",
    "#### Welcome to A3! Please enter answers to the questions in the specified Markdown cells below, and complete the code snippets in the associated python files as specified. When you are done with the assignment, follow the instructions at the end of this assignment to submit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objective üå±\n",
    "In this assignment, you will gain experience extracting and transforming **clinical data** into datasets for downstream statistical analysis. You will practice using the **Python üêç** programming language and common time-saving tools in the **Pandas üêº** library that are ideally suited to these tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources üìñ\n",
    "- Pandas Cheat Sheet üêº: [https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "\n",
    "- Relevant publications:\n",
    "\n",
    "  - We will be following along with the cohort building process presented in [\"A targeted real-time early warning score (TREWScore) for septic shock\" by Henry et al.](http://stm.sciencemag.org/content/7/299/299ra122.full) published in Science Translational Medicine in 2015. \n",
    "\n",
    "  - We will also be referring to [\"Epidemiology of severe sepsis in the United States: Analysis of incidence, outcome, and associated costs of care\" by Angus et al.](https://github.com/MIT-LCP/mimic-code/blob/main/mimic-iii/concepts/sepsis/angus2001.pdf) \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Set-Up üêç\n",
    "To begin, we will need to set up an virtual environment with the necessary packages. A virtual environment is a self-contained directory that contains a Python interpreter (aka Python installation) and any additional packages/modules that are required for a specific project. It allows you to isolate your project's dependencies from other projects that may have different versions or requirements of the same packages.\n",
    "\n",
    "For BIOMEDIN 215 Python assignments, we require that you utilize [Miniconda](https://docs.conda.io/en/latest/miniconda.html) to manage your virtual environments. Miniconda is a lightweight version of [Anaconda](https://www.anaconda.com/), a popular Python distribution that comes with many of the packages that are commonly used in data science."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions for setting up your environment using Miniconda:\n",
    "1. If you do not already have Miniconda installed, download and install the latest version for your opperating system from the following link: [https://docs.conda.io/en/latest/miniconda.html#latest-miniconda-installer-links](https://docs.conda.io/en/latest/miniconda.html#latest-miniconda-installer-links)\n",
    "\n",
    "2. Create a new virtual environment for this assignment by running the following command in your terminal:\n",
    "\n",
    "   ```bash\n",
    "   conda env create -f environment.yml\n",
    "   ```\n",
    "\n",
    "   This will create a new virtual environment called `biomedin215` with Python version 3.10\n",
    "\n",
    "3. Activate your new virtual environment by running the following command in your terminal:\n",
    "\n",
    "   ```bash\n",
    "   conda activate biomedin215\n",
    "   ```\n",
    "\n",
    "   This will activate the virtual environment you created in the previous step.\n",
    "\n",
    "4. Finally, ensure that your `ipynb` (this notebook)'s kernel is set to utilize\n",
    "the `biomedin215` virtual environment you created in the previous steps. Depending on\n",
    "which IDE you are using to run this notebook, the steps to do this may vary.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:08:51.914841Z",
     "start_time": "2024-10-16T23:08:51.899227Z"
    }
   },
   "source": [
    "# Run this cell: \n",
    "# The lines below will instruct jupyter to reload imported modules before \n",
    "# executing code cells. This enables you to quickly iterate and test revisions\n",
    "# to your code without having to restart the kernel and reload all of your \n",
    "# modules each time you make a code change in a separate python file.\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:08:52.212574Z",
     "start_time": "2024-10-16T23:08:51.925482Z"
    }
   },
   "source": [
    "# Run this cell to ensure the environment is setup properly\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"Sanity check: Success\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: Success\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description üìÇ\n",
    "\n",
    "We will be utilizing a subset of the [MIMIC III database](https://mimic.mit.edu/docs/iii/about/), a publically available database of de-identified electronic health records from patients admitted to Intensive Care Units (ICUs) of the [Beth Israel Deaconess Medical Center](https://www.bidmc.org/) in Boston, Massachusetts. MIMIC III is widely used for research and benchmark evaluations in the field of clinical informatics.\n",
    "\n",
    "You will analyze the available data to identify a cohort of patients that underwent septic shock during their admission to the ICU. **All of the data you need for this assignment is available on Canvas.** \n",
    "\n",
    "Once you have downloaded and unzipped the data, you should see the following `6` csv files:\n",
    "- `hypotension_labels.csv`\n",
    "\n",
    "- `vitals_cohort_sirs.csv`\n",
    "\n",
    "- `notes_small_cohort_v2.csv`\n",
    "\n",
    "- `fluids_all.csv`\n",
    "\n",
    "- `labs_cohort.csv`\n",
    "\n",
    "- `diagnoses.csv`\n",
    "\n",
    "**Specify the location of the folder containing the data in the following cells:**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:08:52.333025Z",
     "start_time": "2024-10-16T23:08:52.321767Z"
    }
   },
   "source": [
    "# Specify the path to the folder containing the data files\n",
    "data_dir = \"/Users/stevenang/Documents/stanford/biomedin215/assignments/A3/data\" # <-- TODO: You will need to change this path"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:08:52.453389Z",
     "start_time": "2024-10-16T23:08:52.440971Z"
    }
   },
   "source": [
    "# Run this cell to make sure all of the files are in the specified folder\n",
    "expected_file_list = [\"hypotension_labels.csv\",\n",
    "                      \"vitals_cohort_sirs.csv\",\n",
    "                      \"notes_small_cohort_v2.csv\",\n",
    "                      \"fluids_all.csv\",\n",
    "                      \"labs_cohort.csv\",\n",
    "                      \"diagnoses.csv\"]\n",
    "\n",
    "for file in expected_file_list:\n",
    "    assert os.path.exists(os.path.join(data_dir, file)), \"Can't find file {}\".format(file)\n",
    "\n",
    "print(\"Sanity check: Success\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: Success\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building a Cohort Based on Inclusion Criteria and Defining Endpoints üîé"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1.1`: (`5 pts`)\n",
    "\n",
    "> The first part of any patient-level study is to identify a cohort of patients who are relevant to the study, and at what point during their records they became eligible. Typically, this is done with a set of `inclusion criteria`, which, if met, qualify the patient for inclusion in the cohort. In this assignment, we will emulate the inclusion criteria used in the [TREWScore paper](http://stm.sciencemag.org/content/7/299/299ra122.full).\n",
    "\n",
    "\n",
    " Read the first paragraph of the *Materials and Methods - Study Design* in the [TREWScore paper](http://stm.sciencemag.org/content/7/299/299ra122.full).  \n",
    "   \n",
    " - **What criteria did the authors use to determine which patients should enter the study?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color:blue;\"> The authors identified 16,234 distinct patient with the following criterias:\n",
    "  1) Age 15 and above\n",
    "  2) ICU Admission with at least one assessment each of:\n",
    "        * GCS\n",
    "        * BUN\n",
    "        * Hemotocrit,\n",
    "        * Heart rate record in EHR\n",
    "  </span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `1.2` (`5 pts`)\n",
    "\n",
    "> By looking through the [MIMIC documentation](https://mimic.physionet.org/about/mimic/), you should be able to identify which tables in MIMIC contain the data that you need to identify patients that meet the inclusion criteria you described above. If you're stuck, try looking through the [mimic-code repository](https://github.com/MIT-LCP/mimic-code) provided by the MIMIC maintainers to get some ideas as to how to queries work on the database.\n",
    "\n",
    "- **Report which table(s) in MIMIC you would query in order to identify patients that meet the inclusion criteria you identified above**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;\">Here is the mapping of the MIMIC tables to the selection criterias:<br/><table>\n",
    "    <tr>\n",
    "        <th>Inclusion Criteria</th>\n",
    "        <th>MIMIC III Table</th>\n",
    "        <th>Comments</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>GCS</td>\n",
    "        <td>CHARTEVENTS + D_ITEMS</td>\n",
    "        <td>The CHARTEVENTS contains the actual reading of `GCS` related data and we also need `D_ITEMS` to find out itemid for `GCS` to do the filtering</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>BUN</td>\n",
    "        <td>LABEVENTS + D_LABITEMS</td>\n",
    "        <td>The LABEVENTS contains the actual reading of `BUN` and we also need `D_LABITEMS` to find out itemid for `BUN` to do the filtering</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Hemotocrit</td>\n",
    "        <td>LABEVENTS + D_LABITEMS</td>\n",
    "        <td>The LABEVENTS contains the actual reading of `Hemotocrit` and we also need `D_LABITEMS` to find out itemid for `BUN` to do the filtering</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Heart Rate</td>\n",
    "        <td>CHARTEVENTS + D_ITEMS</td>\n",
    "        <td>The CHARTEVENTS contains the actual reading of `Heart Rate` and we also need `D_ITEMS` to find out itemid for `Heart Rate` to do the filtering</td>\n",
    "    </tr>\n",
    "</table></span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1.3` (`5 pts`) \n",
    "\n",
    "It can be tricky to develop the SQL queries necessary to extract the cohort of interest. Fortunately, the course staff ran the necessary query on the MIMIC III database to extract the identifiers of patients that meet the inclusion criteria discussed above. (There will be an in-class exercise where you can practice running queries later in the course!)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the vitals and labs data for our cohort stored in `vitals_cohort_sirs.csv` and `labs_cohort.csv` into dataframes by running the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:04.846557Z",
     "start_time": "2024-10-16T23:08:52.591428Z"
    }
   },
   "source": [
    "# Run this cell to load the data from the CSV files into Pandas DataFrames\n",
    "labs_cohort = pd.read_csv(os.path.join(data_dir, \"labs_cohort.csv\"))\n",
    "vitals_cohort_sirs = pd.read_csv(os.path.join(data_dir, \"vitals_cohort_sirs.csv\"))"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to examine the dataframes you just created."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:04.861924Z",
     "start_time": "2024-10-16T23:09:04.850591Z"
    }
   },
   "source": [
    "# display(labs_cohort.head())\n",
    "# print(labs_cohort.info())"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:04.886150Z",
     "start_time": "2024-10-16T23:09:04.875037Z"
    }
   },
   "source": [
    "# display(vitals_cohort_sirs.head())\n",
    "# print(vitals_cohort_sirs.info())"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Briefly describe the contents of each dataframe.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": "<span style=\"color:blue;\"> Both dataframe contains some common items like subject_id, hadm_id, icustay_id, and charttime. Where subject_id is unique to patient, hadm_id is unique to a patient hospital stay, icustay_id is unique to patient ICU stay, and charttime records the time at which an observation was charted or recorded.<br/><br/> Dataframe \"labs_cohort\" records all lab events happened to the patient. lab_id contains the name of the laboratory procedures and valuenum contains the actual result of that particular laboratory exam. <br/><br/>On the otherhand, \"vitals_cohort_sirs\" contains patient's vital reading like Heart Rate and Blood Pressure. Similar to \"labs_cohort\", \"vitals_cohort_sirs\" has vital_id describing the vital sign of the record and valuenum field recording the actual value of the reading.</span>"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1.4` (`5 pts`)\n",
    "\n",
    ">While we are ultimately interested in utilizing all of the data from all of the patients that meet our inclusion criteria, it is good practice to work with a small *development set* of data for the purposes of developing our analytical pipeline, so that we may test ideas quickly without having to wait for code to execute on large datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.4.1 Coding Exercise: `get_dev_cohort_list`\n",
    "Congratulations, you made it to the first coding exercise of the course! üéâ\n",
    "\n",
    "In the src folder, you will find a file called `cohort.py` that contains a function called\n",
    "`get_dev_cohort_list`. \n",
    "\n",
    "Complete the function following the instructions in the docstring. When you have completed the function, create a development cohort list from the vitals dataframe by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:04.957564Z",
     "start_time": "2024-10-16T23:09:04.900446Z"
    }
   },
   "source": [
    "# Run this cell after you have completed the get_dev_cohort_list in A3/src/cohort.py\n",
    "from src.cohort import get_dev_cohort_list\n",
    "\n",
    "cohort_list = get_dev_cohort_list(labs_cohort, num_subject_ids=1000)\n",
    "\n",
    "# Sanity Check:\n",
    "assert len(cohort_list) == 1000, f\"Expected 1000 subject_ids, got {len(cohort_list)}\"\n",
    "assert cohort_list[0] == 3, f\"Expected first subject_id to be 3, got {cohort_list[0]}\"\n",
    "assert cohort_list[-1] == 1390, f\"Expected last subject_id to be 1390, got {cohort_list[-1]}\"\n",
    "print(\"Sanity check: Success\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: Success\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.4.2 Coding Exercise: `filter_df`\n",
    "Now that we have our development cohort list, we can use it to filter the dataframes we created above to create development cohort versions of both dataframes.\n",
    "\n",
    "Complete the function `filter_df` in `cohort.py` following the instructions in the docstring. When you have completed the function, create development versions of the `vitals_cohort_sirs` and `labs_cohort` dataframes by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:05.215151Z",
     "start_time": "2024-10-16T23:09:05.018681Z"
    }
   },
   "source": [
    "# Run this cell after you have completed the filter_df in A3/src/cohort.py\n",
    "# NOTE: you do not need to modify the code in this cell\n",
    "from src.cohort import filter_df\n",
    "\n",
    "# Filter the DataFrame\n",
    "dev_vitals = filter_df(vitals_cohort_sirs, \"subject_id\", cohort_list)\n",
    "dev_labs = filter_df(labs_cohort, \"subject_id\", cohort_list)\n",
    "\n",
    "# Sanity Check:\n",
    "assert len(dev_vitals) == 599024, f\"Expected 599024 rows, got {len(dev_vitals)}\"\n",
    "assert len(dev_labs) == 295845, f\"Expected 295845 rows, got {len(dev_labs)}\"\n",
    "print(\"Sanity check: Success\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: Success\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1.5` (`10 pts`)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`Systemic Inflammatory Response Syndrome` (`SIRS`) criteria](https://www.ncbi.nlm.nih.gov/pubmed/1303622) has been an integral tool for the clinical definition of sepsis for the past several decades. If you are interested in learning more about SIRS and some associated controversies surrounding its continued use, read the following article: [Systemic Inflammatory Response Syndrome Criteria in Defining Severe Sepsis](https://www.nejm.org/doi/full/10.1056/NEJMoa1415236#t=article)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in our process will be to assess whether patients satisfy each of the SIRS criteria at *each time step that vitals or lab data is available*. To this end, we would like to have a dataframe where each row corresponds to a unique combination of `subject_id`, `hadm_id`, `icustay_id`, and `charttime`, and with one column for each unique type of lab or vital that was measured at that time. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.5.1 Coding Exercise: `mean_summarization`\n",
    "\n",
    "Some patients have multiple measurements of a given vital or lab that were taken at the same time. There may also have been multiple types of raw measurement that were mapped to the same measurement label.\n",
    "\n",
    "Run the following cell to see an example of this:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:05.267406Z",
     "start_time": "2024-10-16T23:09:05.256363Z"
    }
   },
   "source": [
    "# Run this cell.\n",
    "# NOTE: you do not need to modify the code in this cell\n",
    "\n",
    "# This line displays all HEMATOCRIT labs for the patient with subject_id 34\n",
    "# display(dev_labs[(dev_labs['subject_id'] == 34) & (dev_labs['lab_id'] == 'HEMATOCRIT')].head())\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output of the above cell, there should be two `HEMATOCRIT` measurements recorded at the same time for the patient with `subject_id` 34.\n",
    "\n",
    "This type of issue is very common in EHR data, as healthcare data is notoriously messy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a single value to assess the SIRS criteria at each timepoint for each relevant lab, so we will need to write a function that summarizes the data for us.\n",
    "\n",
    "There are many ways we could potientially do this. Some commonly used approaches include:\n",
    "- Summarize duplicate measurements through an operation (such as taking the mean or median)\n",
    "- Pick a single measurement at random from the set of duplicate measurements for each time point\n",
    "- Pick the measurement with the highest (or lowest) value from the set of duplicate measurements for each time point\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, you will implement a function that summarizes values by calculating the **mean** value for each patient at a given time point. \n",
    "\n",
    "Complete the function `summarize_by_mean` in `features.py` file following the instructions in the docstring. When you have completed the function, use it to summarize the labs and vitals data for each patient timepoint by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:05.801529Z",
     "start_time": "2024-10-16T23:09:05.297389Z"
    }
   },
   "source": [
    "# Run this cell after you have completed the summarize_by_mean in A3/src/features.py\n",
    "# NOTE: you do not need to modify the code in this cell\n",
    "from src.features import summarize_by_mean\n",
    "\n",
    "# Summarize the labs by mean\n",
    "dev_labs_mean = summarize_by_mean(dev_labs, columns_to_group_by=[\"subject_id\",\"hadm_id\",\"icustay_id\",\"charttime\",\"lab_id\"], column_to_summarize=\"valuenum\")\n",
    "\n",
    "# Summarize the vitals by mean\n",
    "dev_vitals_mean = summarize_by_mean(dev_vitals, columns_to_group_by=[\"subject_id\",\"hadm_id\",\"icustay_id\",\"charttime\",\"vital_id\"],column_to_summarize=\"valuenum\")\n",
    "\n",
    "# Sanity Check:\n",
    "assert len(dev_labs_mean) == 295769, f\"Expected 295769 rows, got {len(dev_labs_mean)}\"\n",
    "assert len(dev_vitals_mean) == 579157, f\"Expected 579157 rows, got {len(dev_vitals_mean)}\"\n",
    "print(\"Sanity check: Success\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: Success\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.5.2 Coding Exercise: `pivot_wide`\n",
    "\n",
    "Since we eventually want to assess whether patients satisfy the SIRS criteria at each time point, we would like to have a dataframe where each row corresponds to a unique combination of `subject_id`, `hadm_id`, `icustay_id`, and `charttime`, and contains one column for each unique type of lab or vital. Instead of having a `lab_id` (or `vital_id`) column to indicate what data we are working with and a `value_num` column to indicate the value, we want to have separate columns for each `lab_id` (or `vital_id`) that contain the corresponding value.\n",
    "\n",
    "\n",
    "This is often referred to as a \"wide\" format of a table, since the dataframe becomes much wider by adding a column for each unique type of lab or vital. It is often advantageous to have data in a \"wide\" format for machine learning, as different weights and biases are associated with different features (which will be stored in the columns).\n",
    "\n",
    "\n",
    "Implement the function `pivot_wide` in `features.py` file following the instructions in the docstring. When you have completed the function, create wide format versions of the mean-summarized vitals and labs data by running the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:06.468259Z",
     "start_time": "2024-10-16T23:09:05.841095Z"
    }
   },
   "source": [
    "# Run this cell after you have completed the pivot_wide in A3/src/features.py\n",
    "# NOTE: you do not need to modify the code in this cell\n",
    "from src.features import pivot_wide\n",
    "\n",
    "# Pivot the labs\n",
    "dev_labs_pivot = pivot_wide(dev_labs_mean, index_columns=[\"subject_id\",\"hadm_id\",\"icustay_id\",\"charttime\"], columns=\"lab_id\", values=\"valuenum\")\n",
    "\n",
    "# Pivot the vitals\n",
    "dev_vitals_pivot = pivot_wide(dev_vitals_mean, index_columns=[\"subject_id\",\"hadm_id\",\"icustay_id\",\"charttime\"], columns=\"vital_id\", values=\"valuenum\")\n",
    "\n",
    "# Sanity Check: Number of columns\n",
    "assert len(dev_labs_pivot.columns) == 24, f\"Expected 24 columns in dev_labs_pivot, got {len(dev_labs_pivot.columns)}\"\n",
    "assert len(dev_vitals_pivot.columns) == 8, f\"Expected 8 columns in dev_vitals_pivot, got {len(dev_vitals_pivot.columns)}\"\n",
    "print(\"Sanity check: Success\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: Success\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1.6` (`5 pts`)\n",
    "\n",
    "Since the measurement times for the vitals and labs may be different, the next step is to merge the vitals and labs dataframes together to get a complete timeline for each patient.\n",
    "\n",
    "Implement the function `merge_dataframes` in `features.py` file following the instructions in the docstring. When you have completed the function, create a joined version of the wide format vitals and labs dataframes by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:06.684623Z",
     "start_time": "2024-10-16T23:09:06.509663Z"
    }
   },
   "source": [
    "# Run this cell after you have completed the pivot_wide in A3/src/features.py\n",
    "# NOTE: you do not need to modify the code in this cell\n",
    "from src.features import merge_dataframes\n",
    "\n",
    "dev_merged = merge_dataframes(dev_labs_pivot, dev_vitals_pivot)\n",
    "\n",
    "assert dev_merged.shape[0] == 229577, f\"Your dev_merged dataframe should have 229577 rows, but it has {dev_merged.shape[0]}\"\n",
    "assert dev_merged.shape[1] == 28, f\"Your dev_merged dataframe should have 28 columns, but it has {dev_merged.shape[1]}\"\n",
    "\n",
    "print(\"Sanity check: Success\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: Success\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1.7` (`5 pts`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the resulting dataframe contains a lot of \"missing\" values recorded as `NA`. There are many potential approaches for handling missing values that we could take to address this issue. In this case, we are going to use a last-value-carried-forward approach within an ICU stay to impute missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "**In a sentence or two, describe the potential benefits of using the last-value-carried-forward approach.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<span style=\"color:blue;\"> This approach preserves the sample size and can be useful in time series analysis, specially when data is missing at random or when maintaining the most recent known value is a reasonable assumption </span>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "**In a sentence or two, describe the potential drawbacks of using the last-value-carried-forward approach.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<span style=\"color:blue;\"> In case of the missing data is not random or long period of missing data, the last-value-carried-forward approach assumes no changes occurrs during these gaps. This assumption is very dangerous because it might introduce bias by artificially reducing variability of the feature and potetially overestimating stability in the data which may leads to inaccurate conclusion </span>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `impute_missing` in `features.py` file following the instructions in the docstring. When you have completed the function, impute missing values in the merged vitals and labs dataframe by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:07.617286Z",
     "start_time": "2024-10-16T23:09:06.764063Z"
    }
   },
   "source": [
    "# Run this cell after you have completed the impute_missing in A3/src/features.py\n",
    "# NOTE: you do not need to modify the code in this cell\n",
    "from src.features import impute_missing\n",
    "\n",
    "# Impute missing values\n",
    "dev_imputed = impute_missing(dev_merged)\n",
    "\n",
    "# Assert there are no missing values as a sanity check\n",
    "assert dev_imputed.isnull().sum().sum() == 565949 , \"There is an incorrect number of missing values in the DataFrame.\"\n",
    "\n",
    "print(\"Sanity check: Success\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: Success\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1.8` (`5 pts`)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the TREWScore paper, the authors considered a patient to have sepsis **if at least two of the four SIRS criteria were simultaneously met** during an admission where a suspicion of infection was also present.\n",
    "\n",
    "The four SIRS criteria are as follows:\n",
    "\n",
    "1. A body temperature of under 36 &deg;C or over 38 &deg;C\n",
    "\n",
    "2. Heart rate measured at over 90 beats per minute\n",
    "\n",
    "3. Respiratory rate measured at over 20 breaths per minute `OR` a PaCO2 level of under 32 mmHg\n",
    "\n",
    "4. A white blood cell count:\n",
    "    over 12,000 cells/mm<sup>3</sup> `OR` under 4,000 cells/mm<sup>3</sup> `OR` greater than 10% immature forms (bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We currently have the following data columns for each example:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:07.740409Z",
     "start_time": "2024-10-16T23:09:07.723662Z"
    }
   },
   "source": [
    "# Run this cell to see the names of the columns in the DataFrame:\n",
    "dev_imputed.columns"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject_id', 'hadm_id', 'icustay_id', 'charttime', 'ALBUMIN',\n",
       "       'ANION GAP', 'BANDS', 'BICARBONATE', 'BILIRUBIN', 'BUN', 'CHLORIDE',\n",
       "       'CREATININE', 'GLUCOSE', 'HEMATOCRIT', 'HEMOGLOBIN', 'INR', 'LACTATE',\n",
       "       'PLATELET', 'POTASSIUM', 'PT', 'PTT', 'PaCO2', 'SODIUM', 'WBC',\n",
       "       'HeartRate', 'RespRate', 'SysBP', 'TempC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the following functions in `sirs.py` using the the instructions in the docstrings:\n",
    "- `summarize_sirs`\n",
    "- `get_criteria_1`\n",
    "- `get_criteria_2`\n",
    "- `get_criteria_3`\n",
    "- `get_criteria_4`\n",
    "\n",
    "After implementing the functions, run the following cell to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:07.884793Z",
     "start_time": "2024-10-16T23:09:07.846595Z"
    }
   },
   "source": [
    "# Run this cell after you have completed the summarize_sirs in A3/src/sirs.py\n",
    "# NOTE: you do not need to modify the code in this cell\n",
    "from src.sirs import summarize_sirs\n",
    "\n",
    "\n",
    "# Apply the summarize_sirs function to the DataFrame\n",
    "dev_sirs = summarize_sirs(dev_imputed)\n",
    "\n",
    "# Sanity check the correct number of columns\n",
    "assert dev_sirs.shape[1] == 8, \"The DataFrame has the wrong number of columns\"\n",
    "assert dev_sirs.shape[0] == dev_imputed.shape[0], \"The DataFrame has the wrong number of rows\"\n",
    "\n",
    "print(\"Sanity check: Success\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: Success\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1.9` (`20 pts`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have computed the SIRS criteria for every patient in our cohort. To determine which patients meet the TREWScore definition of sepsis we now also need to determine which patients had suspicion of infection. In the TREWScore paper, the authors use a set of ICD9 codes to identify infection-related diagnoses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have extracted the entirety of the relevant table where ICD9 codes are stored in MIMIC and provide it to you in *diagnoses.csv*. Run the following cell to load the data into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:09.361262Z",
     "start_time": "2024-10-16T23:09:08.000012Z"
    }
   },
   "source": [
    "# Run to read in the diagnoses table from the MIMIC-III database, with all columns as strings:\n",
    "# NOTE: No modifications are needed in this cell\n",
    "diagnoses = pd.read_csv(os.path.join(data_dir, \"diagnoses.csv\"), dtype=str).reset_index()\n",
    "diagnoses.columns = diagnoses.iloc[0]\n",
    "diagnoses = diagnoses.iloc[1:].reset_index(drop=True)\n",
    "diagnoses[\"subject_id\"] = diagnoses[\"subject_id\"].astype(int)\n",
    "diagnoses[\"hadm_id\"] = diagnoses[\"hadm_id\"].astype(int)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:09.468227Z",
     "start_time": "2024-10-16T23:09:09.456385Z"
    }
   },
   "source": [
    "# Run the this cell to see what kind of data is in the diagnosis table.\n",
    "# diagnoses.head()"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the design of the TREWScore paper, we will use two methods to determine if a patient has an infection within a specific admission:\n",
    "\n",
    "- A) An ICD-9 code indicating infection was present during the admission.\n",
    "- B) The word `sepsis` or `septic` appears in a note taken during the admission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `1.9.1` (`10 pts`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will use ICD9 codes to determine which admissions indicate infection was present. Inside of `icd9_processing.py` we have provided lists of ICD9 code prefixes (See `INFECTION_ICD9_PREFIX` in `icd9_processing.py`). Using this reference information, implement the function `summarize_icd9` in `icd9_processing.py` to determine if a given admission has an ICD9 code associated with infection. Follow the instructions in the docstring to implement the function, and run the following cell when you are done to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:09.766716Z",
     "start_time": "2024-10-16T23:09:09.569217Z"
    }
   },
   "source": [
    "# Run this cell after you have completed the summarize_sirs in A3/src/key_icd9_processing.py\n",
    "# NOTE: you do not need to modify the code in this cell\n",
    "from src.icd9_processing import summarize_icd9\n",
    "\n",
    "# Apply the has_infection function to the DataFrame\n",
    "icd9_infections = summarize_icd9(\n",
    "    diagnoses, \n",
    "    subject_ids=cohort_list, \n",
    "    indicator_column_name=\"has_icd9_infection\", \n",
    "    icd9_prefix_list=\"infection\")\n",
    "\n",
    "# Sanity check the correct number of columns\n",
    "assert icd9_infections.shape[1] == 3, \"The DataFrame has the wrong number of columns\"\n",
    "assert icd9_infections.shape[0] == 1378, \"The DataFrame has the wrong number of rows\"\n",
    "\n",
    "print(\"Sanity check: Success\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: Success\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `1.9.2` (`10 pts`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, the authors also consider a patient to have infection during an admission if there is at least one mention of the terms `sepsis` or `septic` in a clinical note taken during their admission. The course staff has done the work of extracting the clinical notes for the 1000 patients we selected for our development set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:10.315294Z",
     "start_time": "2024-10-16T23:09:09.863960Z"
    }
   },
   "source": [
    "notes = pd.read_csv(os.path.join(data_dir,\"notes_small_cohort_v2.csv\"), dtype=str)\n",
    "notes[\"subject_id\"] = notes[\"subject_id\"].astype(int)\n",
    "notes[\"hadm_id\"] = notes[\"hadm_id\"].astype(int)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:10.427543Z",
     "start_time": "2024-10-16T23:09:10.415743Z"
    }
   },
   "source": [
    "# Run this cell to see what the note summary looks like:\n",
    "# notes.head()"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `summarize_notes` in `note_processing.py` following the instructions in the docstring. When you are done, run the following cell to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:10.693765Z",
     "start_time": "2024-10-16T23:09:10.532875Z"
    }
   },
   "source": [
    "from src.note_processing import summarize_notes\n",
    "\n",
    "note_infections = summarize_notes(notes, \"has_note_infection\")\n",
    "\n",
    "# Sanity check\n",
    "assert note_infections.shape[0] == 1373, \"The DataFrame has the wrong number of rows\"\n",
    "assert note_infections.shape[1] == 3, \"The DataFrame has the wrong number of columns\"\n",
    "\n",
    "print(\"Sanity check: Success\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: Success\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1.10` (`5 pts`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! At this stage, we now have all the information we need to determine the times that patients meet the criteria for sepsis as defined in the TREWScore paper. Now we want to join the infection results with the SIRS criteria dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, join the `note_infections` and `icd9_infections` by completing the function `join_infections` in `cohort.py`. Following the implementation instructions in the docstring. Run the following cell to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:10.815251Z",
     "start_time": "2024-10-16T23:09:10.796673Z"
    }
   },
   "source": [
    "from src.cohort import join_infections\n",
    "\n",
    "# Join the infection tables\n",
    "all_infections = join_infections(icd9_infections, note_infections)\n",
    "\n",
    "# Sanity check: Ensure that Nan values are replaced with 0\n",
    "assert all_infections.isnull().sum().sum() == 0, \"There are missing values in the DataFrame\"\n",
    "\n",
    "print(\"Sanity check: Success\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: Success\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to merge `all_infections` with the `dev_sirs` dataframe. Implement the functions `summarize_sepsis` and `get_sepsis_status` in the file `trewscore.py`. Follow the instructions in the docstring of the function, and run the following cell to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:11.003388Z",
     "start_time": "2024-10-16T23:09:10.947902Z"
    }
   },
   "source": [
    "from src.trewscore import summarize_sepsis\n",
    "\n",
    "# Summarize Sepsis\n",
    "dev_sepsis = summarize_sepsis(dev_sirs, all_infections)\n",
    "\n",
    "\n",
    "assert dev_sepsis.shape[0] == 229577\n",
    "assert dev_sepsis.shape[1] == 11\n",
    "\n",
    "print(\"Sanity Check: Success\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check: Success\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all of the sepsis labels! Almost done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1.11` (`10 pts`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the TREWScore paper, the authors also identify patients with **severe sepsis** and **septic shock**. These are the last definitions we need to extract to prepare our cohort for analysis in the next assignment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `1.11.1` (`5 pts`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Severe sepsis is defined as sepsis with **organ dysfunction**. Unfortunately, the criteria the authors use to define organ dysfunction is rather cumbersome. Instead of implementing that criteria explicitly, we adopt a simpler approach. In the [Angus 2001 paper](https://github.com/MIT-LCP/mimic-code/blob/main/mimic-iii/concepts/sepsis/angus2001.pdf), the authors did just that by defining a set of ICD9 codes as a proxy for sepsis-related organ dysfunction. We have provided a list of ICD9 prefixes to pull the relevant codes.\n",
    "\n",
    "Here we will reuse your implementation of the `summarize_icd9` function to pull the ICD9 information."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:11.162621Z",
     "start_time": "2024-10-16T23:09:11.111658Z"
    }
   },
   "source": [
    "# NOTE: The summarize_icd9 function will not be tested for this step\n",
    "# question, but should work for any set of valid arguments.\n",
    "organ_dys = summarize_icd9(diagnoses, \n",
    "    subject_ids=cohort_list, \n",
    "    indicator_column_name=\"has_organ_dysfunction\", \n",
    "    icd9_prefix_list=\"organ_disfunction\")"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the `has_organ_dysfunction` column, we can extract the `severe_sepsis_status`. Implement `summarize_severe_sepsis` in `trewscore.py`. Run the following cell to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:11.304166Z",
     "start_time": "2024-10-16T23:09:11.273182Z"
    }
   },
   "source": [
    "from src.trewscore import summarize_severe_sepsis\n",
    "\n",
    "\n",
    "# Get the TREWScore Severe Sepsis status for each subject at each charttime\n",
    "dev_severe_sepsis = summarize_severe_sepsis(dev_sepsis, organ_dys)\n",
    "\n",
    "assert dev_severe_sepsis.shape[0] == 229577, \"dev_severe_sepsis should have 229577 rows\"\n",
    "assert dev_severe_sepsis.shape[1] == 13, \"dev_severe_sepsis should have 13 columns\"\n",
    "\n",
    "print(\"Sanity Check: Success\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check: Success\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `1.11.2` (`5 pts`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `septic shock` is defined as **severe sepsis**, **hypotension**, and **adequate fluid resuscitation** occurring at the same time.  In order to determine which patients met the criteria for `septic shock` according to the TREWScore paper, we will first need to define the concepts **adequate fluid resuscitation** and **hypotension**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The course staff have created a dataset with a variable indicating whether a patient had adequate fluid resuscitation or was hypotensive at each timepoint in their record. These data are stored in **fluids_all.csv**, **hypotension_labels.csv**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:15.905336Z",
     "start_time": "2024-10-16T23:09:11.442099Z"
    }
   },
   "source": [
    "# Run this cell to load the data\n",
    "# NOTE: No modifications are needed in this cell\n",
    "\n",
    "# Load the data and filter to the cohort\n",
    "hypotension_labels = pd.read_csv(os.path.join(data_dir,\"hypotension_labels.csv\"))\n",
    "hypotension_labels = hypotension_labels[hypotension_labels[\"subject_id\"].isin(cohort_list)]\n",
    "fluids_all = pd.read_csv(os.path.join(data_dir, \"fluids_all.csv\"))\n",
    "fluids_all = fluids_all[fluids_all[\"subject_id\"].isin(cohort_list)]\n",
    "\n",
    "# Drop columns that are not needed\n",
    "fluids_all = fluids_all.drop([\"amount_24h\",\"current_amount\",\"relative_amount\"], axis=1)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `summarize_septic_shock` in the file `trewscore.py` following the instructions in the docstring. Run the cell below to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T23:09:16.652612Z",
     "start_time": "2024-10-16T23:09:16.013529Z"
    }
   },
   "source": [
    "from src.trewscore import summarize_septic_shock\n",
    "\n",
    "# Run this cell after you have completed the summarize_septic_shock in A3/src/key_trewscore.py\n",
    "dev_septic_shock = summarize_septic_shock(dev_severe_sepsis, hypotension_labels, fluids_all)\n",
    "\n",
    "# Sanity Check:\n",
    "assert dev_septic_shock.shape[0] == 239571, \"dev_septic_shock should have 239571 rows\"\n",
    "\n",
    "print(\"Sanity Check: Success\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check: Success\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've extracted a patient cohort from MIMIC and derived multiple sepsis-related endpoints. \n",
    "\n",
    "You're done with assignment 3! ‚úÖ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback (`0 points`)\n",
    "Please fill out the following [feedback form](https://docs.google.com/forms/d/e/1FAIpQLSevaufyQLf5HAFTStk15OJ5idA5OkdLDMsEp8v-fSoPlXKxow/viewform?usp=sf_link) so we can improve the course for future students!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions ‚úÖ\n",
    "\n",
    "There are two files you must submit for this assignment:\n",
    "\n",
    "1. A `PDF` of this notebook.\n",
    "- **Please clear any large cell outputs from executed code cells before creating the PDF.**\n",
    "    - Including short printouts is fine, but please try to clear any large outputs such as dataframe printouts. This makes it easier for us to grade your assignments!\n",
    "- To export the notebook to PDF, you may need to first create an HTML version, and then convert it to PDF.\n",
    "\n",
    "2. A `zip` file containing your code generated by the provided `create_submission_zip.py` script:\n",
    "- Open the `create_submission_zip.py` file and enter your SUNet ID where indicated.\n",
    "- Run the script via `python create_submission_zip.py` to generate a file titled `<your_SUNetID>_submission.zip` in the root project directory.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
