{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4 Feature Engineering [𐄡]\n",
    "\n",
    "#### Welcome to A4! \n",
    "\n",
    "Please enter answers to the questions in the specified Markdown cells below, and complete the code snippets in the associated python files as specified. When you are done with the assignment, follow the instructions at the end of this assignment to submit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objective 🌱\n",
    "In this assignment, you will gain experience transforming clinical data into sets of features for downstream statistical analysis, utilizing the cohort that you developed in A3. In particular, you will extract features from vitals, diagnosis codes, and more that can be used to predict the future development of septic shock. You will practice using common time-saving tools in the **Pandas 🐼** library and **Python 🐍** programming language that are ideally suited to these tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources 📖\n",
    "- Pandas Cheat Sheet 🐼: [https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "\n",
    "- Relevant publications:\n",
    "\n",
    "  - You will not be replicating the models presented in [\"A targeted real-time early warning score (TREWScore) for septic shock\" by Henry et al.](http://stm.sciencemag.org/content/7/299/299ra122.full) directly, but we include a link to the paper for your reference.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Set-Up 🐍\n",
    "To begin, we will need to set up an virtual environment with the necessary packages. A virtual environment is a self-contained directory that contains a Python interpreter (aka Python installation) and any additional packages/modules that are required for a specific project. It allows you to isolate your project's dependencies from other projects that may have different versions or requirements of the same packages.\n",
    "\n",
    "In this course, we require that you utilize [Miniconda](https://docs.conda.io/en/latest/miniconda.html) to manage your virtual environments. Miniconda is a lightweight version of [Anaconda](https://www.anaconda.com/), a popular Python distribution that comes with many of the packages that are commonly used in data science."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions for setting up your environment using Miniconda:\n",
    "1. If you do not already have Miniconda installed, download and install the latest version for your opperating system from the following link: [https://docs.conda.io/en/latest/miniconda.html#latest-miniconda-installer-links](https://docs.conda.io/en/latest/miniconda.html#latest-miniconda-installer-links)\n",
    "\n",
    "2. Create a new virtual environment for this assignment by running the following command in your terminal:\n",
    "\n",
    "   ```bash\n",
    "   conda env create -f environment.yml\n",
    "   ```\n",
    "\n",
    "   This will create a new virtual environment called `biomedin215`\n",
    "\n",
    "3. Activate your new virtual environment by running the following command in your terminal:\n",
    "\n",
    "   ```bash\n",
    "   conda activate biomedin215\n",
    "   ```\n",
    "\n",
    "   This will activate the virtual environment you created in the previous step.\n",
    "\n",
    "4. Finally, ensure that your `ipynb` (this notebook)'s kernel is set to utilize\n",
    "the `biomedin215` virtual environment you created in the previous steps. Depending on\n",
    "which IDE you are using to run this notebook, the steps to do this may vary.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T22:44:45.722229Z",
     "start_time": "2024-10-18T22:44:45.705394Z"
    }
   },
   "source": [
    "# Run this cell: \n",
    "# The lines below will instruct jupyter to reload imported modules before \n",
    "# executing code cells. This enables you to quickly iterate and test revisions\n",
    "# to your code without having to restart the kernel and reload all of your \n",
    "# modules each time you make a code change in a separate python file.\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T22:45:30.441124Z",
     "start_time": "2024-10-18T22:45:30.114244Z"
    }
   },
   "source": [
    "# Run this cell to ensure the environment is setup properly\n",
    "# If you get an error, please ensure that the environment was activated for this notebook\n",
    "# Note: You do not need to edit this cell\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "print(\"Imports Successful!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports Successful!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Note to Students:* 📚\n",
    "\n",
    ">Throughout the assignment, we have provided `sanity checks`: small warnings that will alert you when your implementation is different from the solution. Our goal in providing these numbers is to help you find bugs or errors in your code that may otherwise have gone unnoticed. Please note: the sanity checks are just tools we provided to be helpful, and should not be treated as a target to hit. We manually grade each assignment based on the code you submit, and not based on whether you get the exact same numbers as the sanity checks.\n",
    "\n",
    "Even if you are failing the sanity checks, if your implementation is correct with minor errors, you will still receive the majority of the points (if not all)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T22:45:33.185447Z",
     "start_time": "2024-10-18T22:45:33.172490Z"
    }
   },
   "source": [
    "# Run this cell to set up sanity checks warnings\n",
    "# Note: You do not need to change anything in this cell\n",
    "\n",
    "# Creates a custom warning class for sanity checks\n",
    "class SanityCheck(Warning):\n",
    "    pass\n",
    "\n",
    "# Sets up a cosutom warning formatter\n",
    "def custom_format_warning(message, category, filename, lineno, line=None):\n",
    "    if category == SanityCheck:\n",
    "        # Creates a custom warning with orange text\n",
    "        return f'\\033[38;5;208mSanity Check - Difference Flagged:\\n{message}\\033[0m\\n'\n",
    "    \n",
    "    return '{}:{}: {}: {}\\n'.format(filename, lineno, category.__name__, message)\n",
    "\n",
    "# Sets the warning formatter for the entire notebook\n",
    "warnings.formatwarning = custom_format_warning"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description 📂\n",
    "\n",
    "We will be utilizing the same subset of the [MIMIC III database](https://mimic.mit.edu/docs/iii/about/) we utilized in A3: the 1,000 subject development cohort you created previously. You will start with a dataset very similar to what you may have generated at the end of the prior assignment.\n",
    "\n",
    "You will analyze the available data to identify a cohort of patients that underwent septic shock during their admission to the ICU. **All of the data you need for this assignment is available on Canvas.** \n",
    "\n",
    "Once you have downloaded and unzipped the data, you should see the following `7` csv files:\n",
    "- `cohort_labels.csv`\n",
    "\n",
    "- `ADMISSIONS.csv`\n",
    "\n",
    "- `DIAGNOSES_ICD.csv`\n",
    "\n",
    "- `notes_small_cohort_v2.csv`\n",
    "\n",
    "- `snomed_ct_isaclosure.csv`\n",
    "\n",
    "- `snomed_ct_str_cui.csv`\n",
    "\n",
    "- `vitals_small_cohort.csv`\n",
    "\n",
    "**Specify the location of the folder containing the data in the following cells:**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T22:45:37.852863Z",
     "start_time": "2024-10-18T22:45:37.841027Z"
    }
   },
   "source": [
    "# Specify the path to the folder containing the data files\n",
    "data_dir = \"/Users/stevenang/Documents/stanford/biomedin215/assignments/A4/data\" # <-- TODO: You will need to change this path"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T22:45:40.073105Z",
     "start_time": "2024-10-18T22:45:40.060259Z"
    }
   },
   "source": [
    "# Run this cell to make sure all of the files are in the specified folder\n",
    "expected_file_list = [\"cohort_labels.csv\", \"ADMISSIONS.csv\", \"DIAGNOSES_ICD.csv\", \"notes_small_cohort_v2.csv\", \"snomed_ct_isaclosure.csv\", \"snomed_ct_str_cui.csv\",\"vitals_small_cohort.csv\"]\n",
    "\n",
    "for file in expected_file_list:\n",
    "    assert os.path.exists(os.path.join(data_dir, file)), \"Can't find file {}\".format(file)\n",
    "\n",
    "print(\"All files successfully found\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files successfully found\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Defining labels for prediction 🟩🟩🟩🟥🟩🟩🟥🟩"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1.1`: (`10 pts`)\n",
    "\n",
    "Utilizing our version of the 1,000 subject development cohort you created in the previous assignment, in this assignment, your task is to engineer a set of features that will be used as the inputs to a model that will predict:\n",
    "\n",
    "```\n",
    "At 12 hours into an admission, whether septic shock will occur during the remainder of the admission, with at least 3 hours of lead time (the amount of time between when an event is predicted to occur and when it actually occurs).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, let's load in our intial dataframes. \n",
    "- `cohort_labels.csv`: contains the cohort with the various labels we defined in A3. (This dataset will probably look very similar to the dataset you had at the end of A3.)\n",
    "\n",
    "- `ADMISSIONS.csv`: an extract of the ADMISSIONS table from MIMIC-III. This contains information about patient admission events to the hospital."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T22:45:45.362676Z",
     "start_time": "2024-10-18T22:45:45.031677Z"
    }
   },
   "source": [
    "# Run this cell to load the data from the CSV files into Pandas DataFrames\n",
    "# Note: You do not need to change anything in this cell\n",
    "\n",
    "# Reads in the tables from the CSV files\n",
    "cohort_labels = pd.read_csv(os.path.join(data_dir, \"cohort_labels.csv\"))\n",
    "admissions = pd.read_csv(os.path.join(data_dir, \"ADMISSIONS.csv\"))\n",
    "\n",
    "# Sets the column names to be lowercase\n",
    "admissions.columns = [x.lower() for x in admissions.columns]"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T22:45:51.189804Z",
     "start_time": "2024-10-18T22:45:51.177216Z"
    }
   },
   "source": [
    "# Run this cell to view what the first few rows of the cohort_labels table look like\n",
    "# Note: You do not need to change anything in this cell\n",
    "cohort_labels.head(3)\n",
    "print(cohort_labels.dtypes)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject_id         int64\n",
      "hadm_id            int64\n",
      "icustay_id       float64\n",
      "charttime         object\n",
      "sepsis            object\n",
      "severe_sepsis       bool\n",
      "septic_shock        bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T22:45:55.412671Z",
     "start_time": "2024-10-18T22:45:55.391538Z"
    }
   },
   "source": [
    "# Run this cell to view what the first few rows of the admissions table look like\n",
    "admissions.head(3)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   row_id  subject_id  hadm_id            admittime            dischtime  \\\n",
       "0      21          22   165315  2196-04-09 12:26:00  2196-04-10 15:54:00   \n",
       "1      22          23   152223  2153-09-03 07:15:00  2153-09-08 19:10:00   \n",
       "2      23          23   124321  2157-10-18 19:34:00  2157-10-25 14:00:00   \n",
       "\n",
       "  deathtime admission_type         admission_location  \\\n",
       "0       NaN      EMERGENCY       EMERGENCY ROOM ADMIT   \n",
       "1       NaN       ELECTIVE  PHYS REFERRAL/NORMAL DELI   \n",
       "2       NaN      EMERGENCY  TRANSFER FROM HOSP/EXTRAM   \n",
       "\n",
       "          discharge_location insurance language      religion marital_status  \\\n",
       "0  DISC-TRAN CANCER/CHLDRN H   Private      NaN  UNOBTAINABLE        MARRIED   \n",
       "1           HOME HEALTH CARE  Medicare      NaN      CATHOLIC        MARRIED   \n",
       "2           HOME HEALTH CARE  Medicare     ENGL      CATHOLIC        MARRIED   \n",
       "\n",
       "  ethnicity            edregtime            edouttime  \\\n",
       "0     WHITE  2196-04-09 10:06:00  2196-04-09 13:24:00   \n",
       "1     WHITE                  NaN                  NaN   \n",
       "2     WHITE                  NaN                  NaN   \n",
       "\n",
       "                                           diagnosis  hospital_expire_flag  \\\n",
       "0                            BENZODIAZEPINE OVERDOSE                     0   \n",
       "1  CORONARY ARTERY DISEASE\\CORONARY ARTERY BYPASS...                     0   \n",
       "2                                         BRAIN MASS                     0   \n",
       "\n",
       "   has_chartevents_data  \n",
       "0                     1  \n",
       "1                     1  \n",
       "2                     1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>admittime</th>\n",
       "      <th>dischtime</th>\n",
       "      <th>deathtime</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>admission_location</th>\n",
       "      <th>discharge_location</th>\n",
       "      <th>insurance</th>\n",
       "      <th>language</th>\n",
       "      <th>religion</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>edregtime</th>\n",
       "      <th>edouttime</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "      <th>has_chartevents_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>165315</td>\n",
       "      <td>2196-04-09 12:26:00</td>\n",
       "      <td>2196-04-10 15:54:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>DISC-TRAN CANCER/CHLDRN H</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNOBTAINABLE</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>2196-04-09 10:06:00</td>\n",
       "      <td>2196-04-09 13:24:00</td>\n",
       "      <td>BENZODIAZEPINE OVERDOSE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>152223</td>\n",
       "      <td>2153-09-03 07:15:00</td>\n",
       "      <td>2153-09-08 19:10:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>PHYS REFERRAL/NORMAL DELI</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CORONARY ARTERY DISEASE\\CORONARY ARTERY BYPASS...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>124321</td>\n",
       "      <td>2157-10-18 19:34:00</td>\n",
       "      <td>2157-10-25 14:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>TRANSFER FROM HOSP/EXTRAM</td>\n",
       "      <td>HOME HEALTH CARE</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BRAIN MASS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T22:46:00.760905Z",
     "start_time": "2024-10-18T22:46:00.745943Z"
    }
   },
   "source": [
    "# (OPTIONAL TODO:) It is always a good idea to filter out columns that you don't need from DataFrames.\n",
    "# As always, feel free to add code to your notebooks to do this. This is not required for the assignment.\n",
    "# You may want to come back to this later when you are more familiar with the data and know which columns you need.\n",
    "\n",
    "admissions.drop(['deathtime', 'admission_type', 'admission_location', 'discharge_location', 'insurance', 'language', 'religion', 'marital_status', 'ethnicity', 'edregtime', 'edouttime', 'hospital_expire_flag' ], axis=1, inplace=True)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to do some preprocessing. When working with dates in Pandas, it is always a good idea to convert the data to a datetime format. This can help improve performance, memory efficiency, and also allow us to use the many built-in features of Pandas that are only available for datetime objects. Implement the function `preprocess_dates` in the file `src/utils.py` following the instructions in the docstring, to convert specific columns in the input dataframe that contain dates to datetime objects."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:07:32.081529Z",
     "start_time": "2024-10-19T12:07:32.003206Z"
    }
   },
   "source": [
    "# Run this cell after you have completed the necessary code\n",
    "# Note: you do not need to modify the code in this cell\n",
    "from src.utils import preprocess_dates\n",
    "\n",
    "preprocess_dates(admissions, [\"admittime\", \"dischtime\"], [\"%Y-%m-%d %H:%M:%S\"]*2, inplace=True)\n",
    "preprocess_dates(cohort_labels, [\"charttime\"], [\"%Y-%m-%dT%H:%M:%SZ\"],inplace=True)\n",
    "\n",
    "#==============\n",
    "# Sanity Checks\n",
    "if not \"datetime\" in str(admissions[\"admittime\"].dtype).lower():\n",
    "    warnings.warn(\"The admittime column is not a datetime object\", SanityCheck)\n",
    "\n",
    "if not admissions[\"admittime\"].dt.tz is not None:\n",
    "    warnings.warn(\"The admittime column is not in UTC\", SanityCheck)"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will derive the **labels** and **index times** in a way that aligns with the task description above. Note that we are no longer following the same procedure as the TREWScore paper.\n",
    "\n",
    "We will use the following definitions:\n",
    "\n",
    "* We will only assign labels to admissions of at least twelve hours in duration.\n",
    "\n",
    "* An admission is assigned a negative label if septic shock does not occur at any time during the admission.\n",
    "\n",
    "* An admission is assigned a positive label if septic shock occurs fifteen hours after admission or later.\n",
    "\n",
    "* Admissions where the earliest time of septic shock occurs prior to fifteen hours after admission are removed from the study.\n",
    "\n",
    "* For admissions that have valid labels, we assign an index time at twelve hours into the admission. For prediction, we only use information that occurs before the index time.\n",
    "\n",
    "* In the case that a patient has multiple admissions for which a valid index time and label may be assigned, we only use the latest one.\n",
    "\n",
    "We will use the above definitions to derive the binary classification labels for septic shock and the corresponding index times for each patient in the dataframe. Our goal is to end up with a dataframe that contains a row for each patient in `cohort_labels` that passed the inclusion criteria, with the following columns:\n",
    "- `subject_id`: the unique identifier for each patient\n",
    "- `hadm_id`: the unique identifier for the admission\n",
    "- `label`: the binary classification label for septic shock\n",
    "- `index_time`: the index time for the patient (+12 hours from admission start time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, we do not want to assign labels to admissions that are less than twelve hours in duration. Implement the function `filter_admissions` in `src/labels.py` following the instructions in the docstring and run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:15:36.766856Z",
     "start_time": "2024-10-19T12:15:36.750648Z"
    }
   },
   "source": [
    "# Run this cell after you have completed the necessary code\n",
    "# Note: you do not need to modify the code in this cell\n",
    "from src.labels import filter_admissions\n",
    "\n",
    "filtered_admissions = filter_admissions(admissions)\n",
    "\n",
    "#==============\n",
    "# Sanity Checks\n",
    "\n",
    "# Check that the number of rows is correct\n",
    "if filtered_admissions.shape[0] != 57925:\n",
    "    warnings.warn(\"Number of rows is different than expected\", SanityCheck)"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step will be to merge the two dataframes together, and create two additional columns:\n",
    "- `relative_charttime`: The amount of time between the charttime and the start of the admission\n",
    "- `index_time`: The time at which a prediction will be made (12 hours after the start of the admission)\n",
    "\n",
    "Implement the functions `merge_and_create_times`, `get_relative_charttime`, and `get_index_time` in `src/labels.py` following the instructions in the appropriate docstrings. When you are done, run the cell below to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:20:46.000618Z",
     "start_time": "2024-10-19T12:20:45.947843Z"
    }
   },
   "source": [
    "# Run this cell after you have completed the necessary code\n",
    "# Note: you do not need to modify the code in this cell\n",
    "from src.labels import merge_and_create_times\n",
    "\n",
    "merged_cohort = merge_and_create_times(cohort_labels,filtered_admissions)\n",
    "\n",
    "#==============\n",
    "# Sanity Checks\n",
    "if merged_cohort.shape[0] != 247610:\n",
    "    warnings.warn(f\"Number of rows is different than expected: shape[0] = {merged_cohort.shape[0]}\", SanityCheck)"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to use this merged dataframe to create a new dataframe that contains the labels utilizing the definitions above. Implement the function `get_shock_labels` in `A4/labels.py` following the instructions in the docstring to create a new dataframe with a binary septic shock label for each patient."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:20:51.517209Z",
     "start_time": "2024-10-19T12:20:51.414614Z"
    }
   },
   "source": [
    "# Run this cell after you have completed the necessary code\n",
    "# Note: you do not need to modify the code in this cell\n",
    "from src.labels import get_shock_labels\n",
    "\n",
    "shock_labels = get_shock_labels(merged_cohort)\n",
    "\n",
    "\n",
    "#==============\n",
    "# Sanity Check:\n",
    "for col in shock_labels.columns:\n",
    "    if col not in [\"subject_id\", \"hadm_id\", \"admittime\", \"dischtime\", \"index_time\", \"label\", \"icustay_id\"]:\n",
    "        warnings.warn(f\"Expected column {col} not found\", SanityCheck)\n",
    "    \n",
    "if len(shock_labels) != 974:\n",
    "    warnings.warn(f\"Expected length different: length = {len(shock_labels)}\", SanityCheck)\n",
    "\n",
    "if len(shock_labels) != shock_labels[\"subject_id\"].nunique():\n",
    "    warnings.warn(f\"Expected no duplicate rows\", SanityCheck)"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:20:55.122697Z",
     "start_time": "2024-10-19T12:20:55.109102Z"
    }
   },
   "source": [
    "# Run this cell to see the class balance of the labels:\n",
    "# Note: you do not need to modify the code in this cell\n",
    "shock_labels[\"label\"].value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "False    905\n",
       "True      69\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Feature engineering [𐄡]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have derived labels and index times for each patient in our cohort, we can start to engineer some features from the data that occur prior to the index times and will be useful for predicting onset of septic shock."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets deal with diagnoses. Load in the `DIAGNOSES_ICD.csv` file by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the data from the CSV files into Pandas DataFrames\n",
    "# Note: You do not need to modify the code in this cell\n",
    "\n",
    "# Reads in the table from the CSV file\n",
    "diagnoses = pd.read_csv(os.path.join(data_dir, \"DIAGNOSES_ICD.csv\"))\n",
    "# Sets diagnoses's column names to lower case\n",
    "diagnoses.columns = [x.lower() for x in diagnoses.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.1`: (`2 pts`)\n",
    "\n",
    "Review the documentation for MIMIC to answer the following question.\n",
    "\n",
    "**Which column from which table in MIMIC should you use to find the time of each diagnosis? Justify your response.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color:blue;\"> YOUR ANSWER HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.2`: (`3 pts`)\n",
    "\n",
    "Utilizing the column you selected in the previous question, implement the function `get_diagnoses` in `A4/features.py` following the instructions in the docstring. When you have completed your implementation, run the cell below to sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features import get_diagnoses\n",
    "\n",
    "dx_features = get_diagnoses(admissions, diagnoses, shock_labels)\n",
    "\n",
    "#============\n",
    "# Sanity Check\n",
    "if dx_features.shape[0] != 4031:\n",
    "    warnings.warn(f\"Expected length different: shape[0] = {dx_features.shape[0]}\", SanityCheck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many subjects have diagnoses recorded prior to the index_time? Does the resulting number make sense?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add code to this cell to answer the above question if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color:blue;\"> YOUR ANSWER HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.3`: (`4 pts`)\n",
    "\n",
    "Implement code in the following cell to answer the question\n",
    "\n",
    "**What are the top 10 most common diagnosis codes (by number of unique patients who had the code in their history) in the data frame resulting from question 2.2? Look up the top 3 codes online and report what they refer to.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: IMPLEMENT CODE HERE TO ANSWER THIS QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.4`: (`4 pts`)\n",
    "\n",
    "In this step we will create a histogram for the set of codes and patients that remain after the index time filtering step. \n",
    "\n",
    "Implement the function `show_diagnosis_hist` in `visualize.py` following the instructions in the docstring. When you are done, run the cell below to show the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualize import show_diagnosis_hist\n",
    "\n",
    "# Create the plot\n",
    "show_diagnosis_hist(dx_features, \"diagnosis_count_hist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In 1-2 sentences, interpret the resulting histogram.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color:blue;\"> YOUR ANSWER HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.5`: (`5 pts`)\n",
    "\n",
    "From the histogram you generated earlier, it's evident that there's a substantial variation in the frequency of different diagnoses. Specifically, a significant number of diagnoses appear very infrequently in the dataset.\n",
    "\n",
    "Such a distribution is characteristic of a `sparse feature space`. Here is what that means:\n",
    "\n",
    "`Sparse Feature Space`: In the context of data with categorical variables, a sparse feature space refers to the scenario where many possible features (in this case, diagnosis codes) appear infrequently, resulting in a 'wide form' matrix with many zeros or absent values.\n",
    "\n",
    "This can have problematic implications for downstream analyses:\n",
    "- First, sparse features can pose **computational challenges**: Many machine learning algorithms struggle with high dimensionality and sparsity. They can become computationally intensive or may not work optimally.\n",
    "- Second, sparse features can lead to **issues with generalization**: Rare features often don't contribute significantly to model training. In some cases, they might even introduce noise, making the model overfit to a training set and perform poorly on new, unseen data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these challenges, it's beneficial to address sparsity. One strategy to manage this involves quantifying the \"usefulness\" or \"specificity\" of each feature, and utilizing this information to select features or even perform feature aggregation (grouping features to capture broader patterns). This is where `Information Content (IC)` comes into play:\n",
    "\n",
    "**Definition**: `IC` is a metric that provides a measure of the specificity or the informativeness of a feature based on its frequency of occurrence. Features that are very common have a higher probability and thus a lower IC, while rare features have a lower probability, resulting in a high IC value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IC of a feature that occurs in a set of records is calculated as follows:\n",
    "\n",
    "$IC\\left(\\text{featureA}\\right) = -log_2 \\left( \\frac{count(\\text{Patients with featureA})}{count(\\text{All Patients})} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `calc_ic` in `src/features.py` to calculate the IC of each diagnosis code in the dx_features dataframe using the equation above and following the instructions in the docstring. When you are done, run the cell below to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features import calc_ic\n",
    "\n",
    "icd9_ic = calc_ic(dx_features, all_patients_count=len(shock_labels))\n",
    "\n",
    "if icd9_ic.shape[0] != 914:\n",
    "    warnings.warn(f\"Expected number of rows different: shape[0] = {icd9_ic.shape[0]}\", SanityCheck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.6` (`3 pts`)\n",
    "\n",
    "Use the code cell below to answer the following question:\n",
    "\n",
    "**What is the range (min and max) of ICs observed in your data? What are the 10 most specific ICD9 codes?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: IMPLEMENT CODE HERE TO ANSWER THE QUESTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color:blue;\"> YOUR ANSWER HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.7` (`2 pts`)\n",
    "\n",
    "Now it's time to perform some feature selection. Implement the function `filter_ic` in `src/features.py` to filter the dataframe to only include the diagnoses with an IC between 4 and 9 (inclusive) following the instructions in the docstring. When you are done, run the cell below to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features import filter_ic\n",
    "\n",
    "dx_selected = filter_ic(dx_features, icd9_ic)\n",
    "\n",
    "\n",
    "#============\n",
    "# Sanity Check\n",
    "if dx_selected.shape[0] != 3044:\n",
    "    warnings.warn(f\"Expected number of rows different: shape[0] = {dx_selected.shape[0]}\", SanityCheck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.8` (`12 pts`)\n",
    "\n",
    "Now we have our diagnosis features and the times they occurred for each patient. The next step is to create a patient-feature matrix that summarizes and organizes these diagnosis features. In this matrix, each row should represent a patient and each column should represent a diagnosis code, time-binned by whether or not it occurred in the 6 months prior to the index time.\n",
    "\n",
    "Put simply, for each diagnosis code, we want to generate two features:\n",
    "\n",
    "- One feature representing the count of the number of times the code was observed in the six months prior to the index time.\n",
    "- Another feature for the number of times that code appeared more than six months before the index time.\n",
    "\n",
    "Note that the ICU stay is the first time many patients have been seen at this hospital, so patients may have few or no prior recorded diagnoses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `get_diagnoses_features` in `src/features.py` to create the patient-feature matrix following the instructions in the docstring. When you are done, run the cell below to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features import get_diagnosis_features\n",
    "\n",
    "diagnosis_features = get_diagnosis_features(dx_selected)\n",
    "\n",
    "#============\n",
    "# Sanity Check\n",
    "\n",
    "if diagnosis_features.shape[0] != 209:\n",
    "    warnings.warn(f\"Expected number of rows different: shape[0] = {diagnosis_features.shape[0]}\", SanityCheck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.9` (`4 pts`)\n",
    "\n",
    "Now let's add features from notes. To do so, we'll have to process some text.\n",
    "\n",
    "The `noteevents` table in MIMIC is large and unwieldy, so we've extracted the rows from that table that you will need. The result is in the file `notes_small_cohort_v2.csv`. Let's load this in now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the data from the CSV files into Pandas DataFrames\n",
    "# Note: You do not need to modify the code in this cell\n",
    "\n",
    "# Reads in the table from the CSV file\n",
    "notes = pd.read_csv(os.path.join(data_dir, \"notes_small_cohort_v2.csv\"))\n",
    "\n",
    "# Set notes' column names to lower case\n",
    "notes.columns = [x.lower() for x in notes.columns]\n",
    "\n",
    "# Utilizes the preprocess_dates function to convert the dates to datetime objects\n",
    "preprocess_dates(notes, [\"chartdate\"], [\"%Y-%m-%d\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out what the notes data looks like\n",
    "notes.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the MIMIC database, notes are primarily timestamped using the chartdate column, which captures the date (but not the precise time) when the note was recorded. Another column, charttime, exists, but it is predominantly empty or null for most entries. This presents a challenge when we wish to filter notes based on precise times, such as a patient-specific cutoff time.\n",
    "\n",
    "To address this, our approach will be to filter notes by ensuring that they were recorded strictly before the day corresponding to each patient's `index_time`. This means that if a note's chartdate is the same as the `index_time` (*even if charttime were available*), we would exclude it because we can't ascertain if it was before or after the exact `index_time` time on that day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `filter_by_chartdate` in `src/notes.py` to filter the notes dataframe to only include notes in a patient's record that were recorded before the day corresponding to each patient's `index_time`, following the instructions in the docstring. When you are done, run the cell below to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.notes import filter_by_chartdate\n",
    "\n",
    "notes_filtered = filter_by_chartdate(shock_labels, notes)\n",
    "\n",
    "#============\n",
    "# Sanity Check\n",
    "if notes_filtered.shape[0] != 13213:\n",
    "    warnings.warn(f\"Number of rows differs from expected: shape[0] = {notes_filtered.shape[0]}\", SanityCheck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.10` (`2 pts`)\n",
    "\n",
    "The Unified Medical Language System (UMLS) is a multi-dimensional and dynamic compendium developed by the U.S. National Library of Medicine (NLM) to bridge the gap between various healthcare terminologies and classification systems. At the heart of UMLS lie various terminologies, which provide concept hierarchies as well as sets of terms for individual concepts. For example, there are more than 50 terms in UMLS terminologies for the concept `myocardial infarction`!\n",
    "\n",
    "Here we will use the SNOMED CT (Systematized Nomenclature of Medicine - Clinical Terms): A comprehensive clinical terminology encompassing diseases, clinical findings, procedures, etc. SNOMED CT is a multi-hierarchy system, meaning that each concept can have multiple parents. For example, the concept `myocardial infarction` has two parents: `acute coronary syndrome` and `myocardial disorder`.\n",
    "\n",
    "In this assignment, you will use the SNOMED CT hierarchy and UMLS term sets to construct a dictionary of terms for inflammatory disorders, which you will use to search for associated terms in MIMIC III notes to create additional features.\n",
    "\n",
    "First, load `snomed_ct_isaclosure.csv` and `snomed_ct_str_cui.csv` by running the code in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the data from the CSV files into Pandas DataFrames\n",
    "# Note: You do not need to modify the code in this cell\n",
    "\n",
    "# Reads in tables from the CSV files\n",
    "snomed_ct_isaclosure = pd.read_csv(os.path.join(data_dir, \"snomed_ct_isaclosure.csv\"))\n",
    "snomed_ct_str_cui = pd.read_csv(os.path.join(data_dir, \"snomed_ct_str_cui.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snomed_ct_isaclosure contains the child-parent CUI relationships for all of SNOMED CT.\n",
    "# Note: You do not need to modify the code in this cell\n",
    "snomed_ct_isaclosure.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snomed_ct_str_cui contains the terms (each with a unique term identifier, tid) for each SNOMED CT CUI\n",
    "# Note: You do not need to modify the code in this cell\n",
    "snomed_ct_str_cui.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `merge_snomed` in `src/notes.py` to merge the two dataframes together, following the instructions in the docstring. When you are done, run the cell below to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.notes import merge_snomed\n",
    "\n",
    "# Merge the two tables\n",
    "snomed_ct_concept_str = merge_snomed(snomed_ct_isaclosure, snomed_ct_str_cui)\n",
    "\n",
    "#============\n",
    "# Sanity Check\n",
    "if snomed_ct_concept_str.shape[0] != 16407662:\n",
    "    warnings.warn(f\"Number of rows differs from expected: shape[0] = {snomed_ct_concept_str.shape[0]}\", SanityCheck)\n",
    "\n",
    "if snomed_ct_concept_str.shape[1] != 2:\n",
    "    warnings.warn(f\"Number of columns differs from expected: shape[1] = {snomed_ct_concept_str.shape[1]}\", SanityCheck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.11` (`6 pts`)\n",
    "\n",
    "One feature that is very likely to impact the likelihood of a patient to develop septic shock is whether they currently have or have a history of inflammatory disorders. Let's extract information from clinical notes to look for the presence of this class of disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accomplish this,  implement the function `get_cui_list` in `src/notes.py` to get a list of all the terms that correspond to a CUI in the `snomed_ct_isaclosure` dataframe and that have a specified number of characters or fewer, following the instructions in the docstring. Then, use this function to get a set of terms for `inflammatory disorders` (`C1290884`) that have 20 characters or fewer. How many terms are in the dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.notes import get_cui_list\n",
    "\n",
    "inflammatory_disorder_list = get_cui_list(snomed_ct_concept_str, \"C1290884\", 20)\n",
    "\n",
    "#============\n",
    "# Sanity Check\n",
    "\n",
    "if len(inflammatory_disorder_list) != 2991:\n",
    "    warnings.warn(f\"Length of inflammatory_disorder_list differs from expected: length = {len(inflammatory_disorder_list)}\", SanityCheck)\n",
    "\n",
    "if \"ekc\" != inflammatory_disorder_list[0]:\n",
    "    warnings.warn(f\"First element of inflammatory_disorder_list differs from expected: element = {inflammatory_disorder_list[0]}\", SanityCheck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.12` (`7 pts`)\n",
    "\n",
    "Now let's determine if the notes contain these terms. Implement the function `extract_terms` in `src/notes.py` to search the note text for the terms you collected in the previous step, following the instructions in the docstring. When you are done, run the cell below to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.notes import extract_terms\n",
    "\n",
    "term_df = extract_terms(notes_filtered, inflammatory_disorder_list, 50)\n",
    "\n",
    "#============\n",
    "# Sanity Check\n",
    "if term_df.shape[0] != 13213:\n",
    "    warnings.warn(f\"Number of rows differs from expected: shape[0] = {term_df.shape[0]}\", SanityCheck)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.13` (`6 pts`)\n",
    "\n",
    "Now that we have extracted the terms from the notes and have a representation of which term is in which note in a `wide` dataframe format, we want to determine which concepts are present in each note. To do this, we will reshape the dataframe to a `long` format and normalize terms back to their corresponding concepts. \n",
    "\n",
    "Implement the function `normalize_terms` in `src/notes.py` following the instructions in the docstring. When you are done, run the cell below to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.notes import normalize_terms\n",
    "\n",
    "concept_df = normalize_terms(term_df, snomed_ct_concept_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.14` (`7 pts`)\n",
    "\n",
    "As with the diagnoses, we must transform these concepts data into a patient-feature matrix. Transform `concept_df` into a patient-feature matrix where each row is a patient and each column is the presence or absence of a concept. Here we are not going to do any time binning. Each concept should have only one column. Instead of counts, use a binary indicator to indicate that the concept was present in the patient's notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `get_note_concept_features` in `src/notes.py` following the instructions in the docstring. When you are done, run the cell below to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.notes import get_note_concept_features\n",
    "\n",
    "note_concept_features = get_note_concept_features(concept_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.15` (`2 pts`)\n",
    "\n",
    "Now let's engineer some features from vital sign measurements also relevant to predicting septic shock! Load in the `vitals_small_cohort.csv` file by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the data from the CSV files into Pandas DataFrames\n",
    "# Note: You do not need to modify the code in this cell\n",
    "\n",
    "# Reads in the table from the CSV file\n",
    "vitals = pd.read_csv(os.path.join(data_dir, \"vitals_small_cohort.csv\"))\n",
    "\n",
    "# Preprocess the dates\n",
    "preprocess_dates(vitals, [\"charttime\"], [\"%Y-%m-%dT%H:%M:%SZ\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter the vitals so we are only looking at Heart Rate measurements that were taken prior to the patient's index time.\n",
    "\n",
    "Implement the function `filter_vitals` in `src/vitals.py` to filter the vitals dataframe to only include measurements that were taken prior to the patient's index time, following the instructions in the docstring. When you are done, run the cell below to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vitals import filter_vitals\n",
    "\n",
    "vitals_filtered_hr = filter_vitals(vitals, shock_labels, [\"HeartRate\"])\n",
    "\n",
    "# ============\n",
    "# Sanity Check\n",
    "if vitals_filtered_hr.shape[0] != 9328:\n",
    "    warnings.warn(f\"Number of rows differs from expected: shape[0] = {vitals_filtered_hr.shape[0]}\", SanityCheck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.16` (`4 pts`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets construct some features. One feature of interest might be the latest value of the heart rate before the `index_time`. \n",
    "\n",
    "Implement the function `get_latest_hr` in `src/vitals.py` to get the latest heart rate measurement before the `index_time` for each patient. When you are done, run the cell below to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vitals import get_latest_hr\n",
    "\n",
    "latest_hr_df = get_latest_hr(vitals_filtered_hr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a histogram to look at the distribution of the latest heart rate values.\n",
    "\n",
    "Implement the function `show_hr_hist` in `src/visualize.py` to plot a histogram of the latest heart rate values, following the instructions in the docstring. When you are done, run the cell below to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualize import show_hr_plot\n",
    "\n",
    "# Create the plot\n",
    "show_hr_plot(latest_hr_df, \"hr_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.17` (`4 pts`)\n",
    "\n",
    "There are some additional considerations we should think about prior to utilizing the latest heart rate feature in our model. For example, if the latest recorded heart rate is not very close to the patient's `index_time`, the feature may not be very useful for that patient. \n",
    "\n",
    "To examine this issue, let's plot the distribution of the time between the latest heart rate measurement and the `index_time`. Implement the function `show_hr_time_hist` in `src/visualize.py` to plot a histogram of the time between the latest heart rate measurement and the `index_time`, following the instructions in the docstring. When you are done, run the cell below to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualize import show_hr_time_plot\n",
    "\n",
    "# Create the plot\n",
    "show_hr_time_plot(latest_hr_df, \"hr_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.18` (`5 pts`)\n",
    "\n",
    "Another concern is that when monitoring patients, especially when thinking about heart rate recordings, relying on a single data point can be misleading. By merely using the last recorded value, we run the risk of using an atypical value. Imagine a scenario where a patient's heart rate is regularly around 80 beats per minute, but due to some temporary distress or a device error, the last recorded value spikes to 120 bpm. If we base our analysis or decisions on this single data point, our conclusions will be skewed.\n",
    "\n",
    "To address these concerns, instead of using just the last measurement, we can utilize a more robust metric: the time-weighted average heart rate. The idea behind a time-weighted average is to account for all measurements while giving more weight to recent ones. This ensures that:\n",
    "- All data points contribute to the final value.\n",
    "- More recent data has a higher influence on the average, as it might be more relevant to the patient's current state.\n",
    "\n",
    "Use the formula $w = e^{(-|\\Delta t| - 1)}$ to calculate the weights of each measurement, where $\\Delta t$ is the time difference between the measurement time and the cutoff time in hours. \n",
    "\n",
    "Calculate the weighted average for each patient with the formula $\\bar{x}_w = \\sum(x_i w_i)/\\sum(w_i)$, where $x_i$ is the value of the measurement and $w_i$ is the weight of that measurement, and $i$ ranges from 1 to the total number of measurements for that patient.\n",
    "\n",
    "The result should be a dataframe with two columns: `subject_id` and `time_wt_avg`. Implement the function `get_time_weighted_hr` in `src/vitals.py` to calculate the time-weighted average heart rate for each patient, following the instructions in the docstring. When you are done, run the cell below to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vitals import get_time_weighted_hr\n",
    "\n",
    "time_weighted_hr_df = get_time_weighted_hr(vitals_filtered_hr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.19` (`4 pts`)\n",
    "\n",
    "Let's do a sanity check to see if what we've done makes sense. We expect that the time-weighted average heart rate and the latest recorded heart rate should be similar.\n",
    "\n",
    "Make a scatterplot of the latest recorded heart rate (x-axis) and the time-weighted average heart rate (y-axis) of each patient. Implement the function `show_hr_scatter` in `src/visualize.py` to plot the scatterplot. When you are done, run the cell below to sanity check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualize import show_hr_scatter\n",
    "\n",
    "# Create the plot\n",
    "show_hr_scatter(latest_hr_df, time_weighted_hr_df, \"hr_scatter.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally run this cell to get all the heart rate features together in the same dataframe\n",
    "heart_rate_features = pd.merge(\n",
    "        latest_hr_df.drop(columns=[\"charttime\"]),\n",
    "        time_weighted_hr_df,\n",
    "        on=\"subject_id\",\n",
    "        how=\"inner\",\n",
    "    ).drop(columns=[\"label\", \"index_time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.20` (`4 pts`)\n",
    "\n",
    "We're almost there! Our final patient-feature matrix will simply be the amalgamation of the different feature matrices we've created. Implement `join_and_clean_data` in `src/utils.py` to combine the columns of the feature matrices from diagnoses, notes, and heart rate measurements, following the instructions in the docstring. Note that not all patients have diagnoses or note features, so this function should fill in any NA values with 0 to indicate that there were no diagnoses or notes counted. Similarily, not all subjects have heart rate measurements.  Fill NA values for these features with a simple column mean imputation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import join_and_clean_data\n",
    "\n",
    "joined = join_and_clean_data(diagnosis_features, note_concept_features, heart_rate_features)\n",
    "\n",
    "#============\n",
    "# Sanity Check\n",
    "\n",
    "if joined.shape[0] != 773:\n",
    "    warnings.warn(f\"Number of rows differs from expected: shape[0] = {joined.shape[0]}\", SanityCheck)\n",
    "\n",
    "if joined.isna().sum().sum() != 0:\n",
    "    warnings.warn(f\"Dataframe contains NaN values, which is not expected\", SanityCheck)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `list(joined.columns)` to look at all the features and make sure everything makes sense.\n",
    "\n",
    "**How many total features are there?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add code to this cell to answer question above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `3` Open Ended Feature Engineering - Do something cool! (`20 pts`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having made it this far, you have picked up a few generalizable techniques that can now be used to extract features from various modalities of clinical data. To test the skills you've learned thus far, you now have free rein to get creative and derive whatever additional features you would like and use them alongside the disease, text and vitals features as input to a simple classifier. To help you with your task, we provide you with CSV files for ALL of the tables in MIMIC III where each table has been filtered to contain only the records for the patients in our small cohort. These are stored in the folder `additional_data`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see what data files are available\n",
    "# Note: You do not need to modify the code in this cell\n",
    "\n",
    "# Let's take a look at what files we have available\n",
    "# Note: You do not need to modify the code in this cell\n",
    "file_list = os.listdir(os.path.join(data_dir, \"additional_data\"))\n",
    "file_list.sort()\n",
    "\n",
    "print(\"Available files: ---\")\n",
    "for ind, f in enumerate(file_list):\n",
    "    print(f, end=\" \"*10)\n",
    "    if ind % 3 == 2:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide you with some baseline code below that runs a logistic regression classifier with a Lasso L1 penalty and reports a cross-validation AUC-ROC. Use the code below to see the performance of the model with the features you have already engineered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell (Depending on your computer and your implementation, this cell may take a while to run)\n",
    "# Note: fit_model is a provided function, you do not need to implement it\n",
    "# Note: Your implementation is not expected to hit any performance targets.\n",
    "# With only the features we have defined above, note the results are not be very good.\n",
    "# In future assignments we will take a closer look at models!\n",
    "from src.model import fit_model\n",
    "fit_model(joined, shock_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Use the code to do the following:\n",
    "* Outside of the features we engineered previously in the assignment, derive additional features that utilize at **least five of the additional data tables**. You may use tables that we have previously worked with as a part of the assignment, but we encourage you to explore these new data sources. Caveats: definition tables (e.g. d_items) do not count towards the five and using any combination of chartevents tables counts as a single table.\n",
    "* Combine your derived features into a patient-feature matrix\n",
    "* Adapt the model-fitting code provided above to your new dataset below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load in some data\n",
    "file_name = \"chartevents_7_additional.csv\" # <-- TODO: Change this to any file you want to load\n",
    "additional_data = pd.read_csv(os.path.join(data_dir, \"additional_data\", file_name))\n",
    "\n",
    "# TODO: Repeat the above code for other tables as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement your own feature engineering here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement your own model here\n",
    "\n",
    "# fit_model(joined, shock_labels) # TODO: Uncomment this line to run your model once you have implemented it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write 1-2 paragraphs discussing what and how many features you derived. Additionally, discuss the effects of those features on the performance of the classifier.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color:blue;\"> YOUR ANSWER HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback (`0 points`)\n",
    "Please fill out the following [feedback form](https://docs.google.com/forms/d/e/1FAIpQLSevaufyQLf5HAFTStk15OJ5idA5OkdLDMsEp8v-fSoPlXKxow/viewform?usp=sf_link) so we can improve the course for future students!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions ✅\n",
    "\n",
    "There are two files you must submit for this assignment:\n",
    "\n",
    "1. A `PDF` of this notebook.\n",
    "- **Please clear any large cell outputs from executed code cells before creating the PDF.**\n",
    "    - Including short printouts is fine, but please try to clear any large outputs such as dataframe printouts. This makes it easier for us to grade your assignments!\n",
    "- To export the notebook to PDF, you may need to first create an HTML version, and then convert it to PDF.\n",
    "\n",
    "2. A `zip` file containing your code generated by the provided `create_submission_zip.py` script:\n",
    "- Open the `create_submission_zip.py` file and enter your SUNet ID where indicated.\n",
    "- Run the script via `python create_submission_zip.py` to generate a file titled `<your_SUNetID>_submission_A4.zip` in the root project directory.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
